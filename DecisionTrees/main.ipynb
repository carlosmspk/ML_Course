{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "Decision trees are a fairly easy to understand machine learning model that works by making use of thresholds on the independent variables to achieve classification (binary or multiclass).\n",
    "\n",
    "E.g:\n",
    "two independent variables: x1 and x2\n",
    "\n",
    "three classes for y: 0, 1 and 2\n",
    "\n",
    "Our Decision Tree model could be something as simple as:\n",
    "\n",
    "```\n",
    "if x1 < 3:\n",
    "    y = 2\n",
    "else:\n",
    "    if x2 < 5:\n",
    "        y = 0\n",
    "    else:\n",
    "        y = 1\n",
    "```\n",
    "\n",
    "The way these values for which the independent variables are compared against, are determined through entropy: the measurement of randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.9402859586706311\n",
      "1.584962500721156\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "from math import log2\n",
    "\n",
    "def entropy(values):\n",
    "    result = 0\n",
    "    total = sum(values)\n",
    "    for val in values:\n",
    "        # ignore 0 values as log(0) is undefined\n",
    "        if val == 0:\n",
    "            continue\n",
    "        # apply entropy formula\n",
    "        prob = val/total\n",
    "        result += prob*log2(prob)\n",
    "    result = max(0.0, -result)\n",
    "    return result\n",
    "\n",
    "print (entropy([100, 0])) # No entropy\n",
    "print (entropy([5, 5])) # full entropy\n",
    "print (entropy([9, 5])) # high entropy, but there's a tendency\n",
    "print (entropy([5, 5, 5])) # full entropy for 3 classes\n",
    "\n",
    "#if we want to normalize entropy:\n",
    "def normalized_entropy(values):\n",
    "    result = 0\n",
    "    total = sum(values)\n",
    "    for val in values:\n",
    "        # ignore 0 values as log(0) is undefined\n",
    "        if val == 0:\n",
    "            continue\n",
    "        # apply entropy formula\n",
    "        prob = val/total\n",
    "        result += prob*log2(prob)\n",
    "    # normalize between 0 and 1\n",
    "    result /= log2(1/len(values))\n",
    "    return result\n",
    "\n",
    "print (normalized_entropy([5, 5, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "Entropy is relevant because decision trees use it as criteria for leaf placement. If a given node in the tree has high entropy, than the tree will branch. When a node has no entropy, the node will become a leaf.\n",
    "\n",
    "Intuitively, while the distribution of classification in a given is random, we keep branching, until it becomes \"deterministic\", i.e., non-random.\n",
    "\n",
    "### Information Gain\n",
    "Another important concept, which makes use of entropy is Information Gain (IG), which indicates how much a given variable provides information regarding another.\n",
    "\n",
    "IG(Y,X) = E(Y) - E(Y|X)\n",
    "\n",
    "The above expression is quite intuitive: \"How much entropy did we loose on Y, by acquiring information regarding X\", or, dumbing it down further: \"Now that I know X, how less random is Y?\"\n",
    "\n",
    "If the entropy value is normalized, IG will always be normalized as well. Regardless of normalization, bigger IG is always a good thing. Unitary IG means that Y is completely random by itself, but completely certain if we know X. Thus, IG can be used to evaluate how much a particular variable is informative of our target classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example: whether to play tennis or not, based on categorical wheather data\n",
    "\"\"\"\n",
    "\n",
    "data_dict = {\n",
    "    \"Outlook\" : [\n",
    "        \"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\", \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\", \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\"],\n",
    "    \"Temperature\" : [\n",
    "        \"Hot\", \"Hot\", \"Hot\", \"Mild\", \"Cool\", \"Cool\", \"Cool\", \"Mild\", \"Cool\", \"Mild\", \"Mild\", \"Mild\", \"Hot\", \"Mild\"],\n",
    "    \"Humidity\": [\n",
    "        \"High\", \"High\", \"High\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"Normal\", \"Normal\", \"High\", \"Normal\", \"High\"],\n",
    "    \"Wind\": [\n",
    "        \"Weak\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Weak\", \"Weak\", \"Strong\", \"Strong\", \"Weak\", \"Strong\"],\n",
    "    \"PlayTennis\": [\n",
    "        \"No\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5774062828523454\n",
      "1.5566567074628228\n",
      "1.0\n",
      "0.9852281360342515\n",
      "0.9402859586706311\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for key in data_dict:\n",
    "    print(entropy(Counter(data_dict[key]).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "raw_entropy = 0.9402859586706311\n",
      "\n",
      "       Feature        IG\n",
      "0      Outlook  0.246750\n",
      "2     Humidity  0.151836\n",
      "3         Wind  0.048127\n",
      "1  Temperature  0.029223\n"
     ]
    }
   ],
   "source": [
    "# Let's see, for two variables, the Information Gain we get for PlayTennis\n",
    "\n",
    "# Outlook\n",
    "label_counts = Counter(data_dict[\"PlayTennis\"]).values()\n",
    "raw_entropy = entropy(label_counts)\n",
    "print (f\"\\n{raw_entropy = }\\n\")\n",
    "\n",
    "#for ease of use, let's convert the dictionary into a pandas DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# And now, create a function to calculate IG\n",
    "def information_gain(independent_variable : str, label_variable : str = \"PlayTennis\"):\n",
    "    original_entropy = entropy(label_counts)\n",
    "    relative_entropy = 0\n",
    "    total = len(df)\n",
    "    \n",
    "    for feature, count in Counter(df[independent_variable]).items():\n",
    "        df_subset = df.loc[df[independent_variable] == feature]\n",
    "        label_cntr = Counter(df_subset[label_variable])\n",
    "        entropy_given_feature = entropy(label_cntr.values())\n",
    "        relative_entropy += entropy_given_feature*count/total\n",
    "    return original_entropy-relative_entropy\n",
    "\n",
    "ig_dict = {\n",
    "    \"Feature\" : [],\n",
    "    \"IG\" : []\n",
    "}\n",
    "for feature in df.drop(\"PlayTennis\", axis=1):\n",
    "    ig_dict[\"Feature\"].append(feature)\n",
    "    ig_dict[\"IG\"].append(information_gain(feature))\n",
    "print (pd.DataFrame(ig_dict).sort_values(by=(\"IG\"), ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Outlook PlayTennis\n",
      "2   Overcast        Yes\n",
      "6   Overcast        Yes\n",
      "11  Overcast        Yes\n",
      "12  Overcast        Yes\n",
      "3       Rain        Yes\n",
      "4       Rain        Yes\n",
      "5       Rain         No\n",
      "9       Rain        Yes\n",
      "13      Rain         No\n",
      "0      Sunny         No\n",
      "1      Sunny         No\n",
      "7      Sunny         No\n",
      "8      Sunny        Yes\n",
      "10     Sunny        Yes\n",
      "   Temperature PlayTennis\n",
      "4         Cool        Yes\n",
      "5         Cool         No\n",
      "6         Cool        Yes\n",
      "8         Cool        Yes\n",
      "0          Hot         No\n",
      "1          Hot         No\n",
      "2          Hot        Yes\n",
      "12         Hot        Yes\n",
      "3         Mild        Yes\n",
      "7         Mild         No\n",
      "9         Mild        Yes\n",
      "10        Mild        Yes\n",
      "11        Mild        Yes\n",
      "13        Mild         No\n"
     ]
    }
   ],
   "source": [
    "# Based on the previous result, we can infer that Outlook is the most informative feature, while temperature is near irrelevant. This can be intuitively confirmed, by inspecting the dataframe.\n",
    "\n",
    "# There should be a relationship between \"Yes\" and \"No\", and Outlook feature values:\n",
    "print (df.sort_values(by=\"Outlook\").drop([i for i in df if i not in (\"Outlook\", \"PlayTennis\")], axis=1))\n",
    "# and indeed, \"Outcast\" always results in \"Yes\", while \"Rain\" and \"Sunny\" seem to have less impact\n",
    "\n",
    "# There should NOT be a relationship in the case of Temperature:\n",
    "print (df.sort_values(by=\"Temperature\").drop([i for i in df if i not in (\"Temperature\", \"PlayTennis\")], axis=1))\n",
    "# and indeed, there is no obvious relationship between any temperature value and \"Yes\"/\"No\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity\n",
    "Alternatively to IG, Gini Impurity can be used to acess branching, and is useful due to its conceptual simplicity, and resulting computational speed. It is given as:\n",
    "Igi(Y) = 1 - sum(P(yi)^2)\n",
    "\n",
    "E.g.\n",
    "\n",
    "For a given decision tree, a node has the condition x1 > a in its root node. The possible labels are \"Yes\" and \"No\".\n",
    "\n",
    "In the root node itself (before the condition is applied) the number of \"Yes\" and \"No\" is the same, and thus, Igi(Y) = 1-(P(\"Yes\")^2 + P(\"No\")^2) = 0.5\n",
    "\n",
    "Now, the condition is applied, and we move on to the next node (whatever it may be), and in this new node, the number of \"Yes\" decrease substantially such that its probability is now 0.3. So, now, Igi(Y) = 0.42.\n",
    "\n",
    "In any given Leaf Node, where there are only \"Yes\" or \"No\", we get Igi(Y) = 0 -> we have zero impurity, i.e., the node is \"pure\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-categorical data\n",
    "### Continuous data\n",
    "\n",
    "What if we want to evaluate a continous variable's information gain? Well, we can't, not directly. Instead intervals in the variable can be defined and considered categories.\n",
    "\n",
    "E.g.\n",
    "x1 ∈ [0, 3[\n",
    "\n",
    "x1 can be made categorical through:\n",
    "\n",
    "xc1 = 0, if x1 < 1,\n",
    "\n",
    "xc1 = 1, if 1 <= x1 < 2,\n",
    "\n",
    "xc1 = 2, if 2 <= x1 < 3\n",
    "\n",
    "And thus, xc1 is now a categorical variable with 3 possible values, and the information gain from those values can be calculated.\n",
    "\n",
    "### Integers\n",
    "Although not usual (nor recommended), integers can be used as categorical variables as well. The reason it's not recommended is due to how the label might be under represented: suppose we have x1 being an integer variable over a huge range, such as [1, 1000], with only two possible resulting labels. In the worst case scenario, if all our data points have a unique integer (no number is repeated) than each integer will be directly associated with its corresponding label, leading the entropy to be always 0, and the information gain to be maximal, even though it's likely that our model will perform poorly on future values based on this feature alone. Our model would effectively be storing a one-to-one table of integer:label pairs. However, for the same scenario, if we are examing over 1 million points, then even this naive approach might yield decent results, because it's impossible for this \"one-to-one\" correspondance to happen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decison Trees for Regression\n",
    "Decision trees can also be used for regression, in much the same way as described before for adapting the independent variables. Often, the predicted value is the average of the subset. This means that the resulting prediction is constant within the same branch, resulting in snappy and jittery regression:\n",
    "\n",
    "![Regression Decision Tree](DTreg.png)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eda11d5f245a84b717531ca04dfa671a6ed0fa46cda81e085f0276f9b706bb18"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
