{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning\n",
    "\n",
    "Each machine learning model has its strengths and drawbacks. Ensemble learning aims to use multiple models and somehow combine their predictions to achieve a more robust result. In regression, one can average out the results. In classification problems, a majority voting can be used.\n",
    "\n",
    "## Cases of Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Useful in low bias, high variance models.\n",
    "\n",
    "Bootstrap Aggregation (B-Agg ing) consists in sampling a subset of the training dataset with replacement, and use that subset to train (leading to a trained model, let's call it M1). Do this same procedure as many times as desired, leading to n trained models: M1, M2, ..., Mn. Now, rather than using a single \"best\" model, we keep all obtained models, and fuse their results, by averaging, picking the median, etc. Bagging can also be used for features, meaning we sample only some features, rather than using all features of a data point. Most commonly, Bagging uses both \"row sampling\" (pick a subset of data points) and \"column sampling\" (pick a subse of features) simultaneously.\n",
    "\n",
    "The point of bagging is to reduce variance, by making the model less sensitive to outlier data and overfitting those points. By resampling the data, we increase the probability that these outlier points won't often be used for training in the majority of the results. This, however, increases bias, by \"smoothing\" the decision/regression of the algorithm.\n",
    "\n",
    "E.g. Random Forest (Decision Tree Bagging) -> bagging is done randomly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "Useful in high bias, low variance models.\n",
    "\n",
    "While in bagging we split the dataset and train multiple models in parallel, boosting aims to train multiple models sequentially. The overall gist of it, is that we train a model, verify which points it performed worst (e.g. highest residual error) and somehow focus on that information on the next training. How that information is used, depends on the boosting method.\n",
    "\n",
    "#### Adaboost\n",
    "Adaptive Boosting uses a weighted sum of the weak learners for the final result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d0c2d3d004f76969b46b128b38bbe9f6254a9637d6b8fe6fc9b8f17e69a7d9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
